##這個Coding是利用Python完成

import pandas as pd
import numpy as np

##利用pandas讀取資料

data_train = pd.read_csv('C:/Users/carter840826/01_Hoteltrain.csv')
data_test = pd.read_csv('C:/Users/carter840826/01_Hoteltest.csv')

##複製一個訓練資料

data1 = data_train.copy(deep = True)


##檢查訓練、驗證資料資訊及有無空白

print(data1.head())
print(data1.info())
print("-"*10)
print('Train columns with null values:\n', data_train.isnull().sum())
print("-"*10)
print('Test columns with null values:\n', data_test.isnull().sum())

import jieba

##載入中文斷詞

fp = open(r'C:/Users/carter840826/Chinese stopwords.txt',encoding='utf-8')
stopwords = fp.readlines()
stopwords = [w.replace('\n', '') for w in stopwords]

##將訓練資料集合成新的DataFarme，多添加segment之後放入處理過的句子

corpos = pd.DataFrame({
    'review': data1['review'], 
    'label': data1['label'],
    'segment': 'null'
    })

##載入jieba，利用for迴圈處理每個句子、分詞以及移除斷詞
##由於電腦速度問題，這邊無法使用posseg更精確地以詞性分詞，可能會造成之後正確率的問題

for i in range(len(corpos['review'])):
    segments = []
    segment = []
    label = corpos['label'][i]
    review = corpos['review'][i]
    segs = jieba.cut(review)
    for seg in segs:
        segs = []
        if seg not in stopwords:
            segs = segments.append(seg)
    
    corpos['segment'][i] = segments
    
##將處理好的分詞與label組成新的DataFrame，print出來看看成果
    
segmentDataFrame = pd.DataFrame({
    'segment': corpos['segment'], 
    'label': corpos['label']
    })
print(segmentDataFrame)

import keras

##利用keras將文字轉換為相對應的數值，MAX_NUM_WORDS代表轉換字典內最多能包含的數目

MAX_NUM_WORDS = 12500
tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)

corpus_x = segmentDataFrame.segment
print(pd.DataFrame(corpus_x.iloc[:5],columns=['segment']))

##將處理好的分詞套入上面設定的tokenizer建立字典並轉換為相對應的數值
##print x_train與y_train的長度以確定資料量

tokenizer.fit_on_texts(corpus_x)
x_train = tokenizer.texts_to_sequences(corpus_x)
y_train = segmentDataFrame.label
print(len(x_train),len(y_train))

##列出每句話的轉換後字串長度，並列出前5個數值
##計算字串長度的max值以及mean值

for seq in x_train[:5]:
    print(len(seq), seq[:5], '...')
max_seq_len = max([len(seq) for seq in x_train])
mean_seq_len = sum([len(seq) for seq in x_train])/len([len(seq) for seq in x_train])
print(max_seq_len)
print(mean_seq_len)

##將字串用Zero Padding設定在同樣的字串長度=35
##不足的在前方以0補齊

MAX_SEQUENCE_LENGTH = 35
x_train = keras.preprocessing.sequence.pad_sequences(
    x_train,
    maxlen=MAX_SEQUENCE_LENGTH)

##將y_train轉換為One-hot Encoding，以便之後在決定答案時有機率值作依據

y_train = keras.utils.to_categorical(y_train)


##再確定一次轉換後的句子數值以及答案數值

print(x_train[:5])
print(y_train[:5])

from sklearn.model_selection import train_test_split

##利用sklearn裡面的train_test_split切割訓練資料
##VALIDATION_RATIO=0.1代表切割資料比率為9:1

VALIDATION_RATIO = 0.1
RANDOM_STATE = 10

x_train,x_val,y_train,y_val = train_test_split(
    x_train, 
    y_train, 
    test_size=VALIDATION_RATIO, 
    random_state=RANDOM_STATE)

##確認訓練集合(Trainning Set)以及驗證集合(Validation Set)大小

print("Training Set")
print("-" * 10)
print(f"x_train : {x_train.shape}")
print(f"y_train : {y_train.shape}")
print("-" * 10)
print("Validation Set")
print("-" * 10)
print(f"x_val :   {x_val.shape}")
print(f"y_val :   {y_val.shape}")


from tensorflow.keras import Input
from tensorflow.keras.layers import Embedding, LSTM, concatenate, Dense
from tensorflow.keras.models import Model

##利用keras內的模組建立RNN模型，RNN為有記憶力的循環網路，會依序讀取list裡面的值以後，計算並做出回饋
##利用LSTM作為RNN模型內的細胞，LSTM的優點為可以將記憶狀態儲存
##其中NUM_CLASSES為答案種類(0,1)；NUM_EMBEDDING_DIM為一個詞向量的維度；NUM_LSTM_UNITS為LSTM輸出的向量維度

NUM_CLASSES = 2
MAX_NUM_WORDS = 12500
NUM_EMBEDDING_DIM = 256
NUM_LSTM_UNITS = 128
MAX_SEQUENCE_LENGTH = 35

##設定模型內的Input data以及Output data
##經過詞嵌入層的轉換，每則評論都變成一個詞向量的序列
##而每個詞向量的維度為256

data_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')
embedding_layer = Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM)
data_embedded = embedding_layer(data_input)

##評論經過LSTM層後轉換為一個128維度的向量

shared_lstm = LSTM(NUM_LSTM_UNITS)
data_output = shared_lstm(data_embedded)

##利用Softmax Activation傳回兩個值，分別代表可能的機率

dense =  Dense(units=NUM_CLASSES, activation='softmax')
predictions = dense(data_output)

#建立模型

model = Model(inputs=data_input, outputs=predictions)
print(model.summary())

##定義損失函數的模型
##binary_crossentropy為損失函數內的交叉熵；accuracy可以用來了解模型正確度
##優化器選擇rmsprop

model.compile(
    optimizer='rmsprop',
    loss='binary_crossentropy',
    metrics=['accuracy'])
		
BATCH_SIZE = 128
NUM_EPOCHS = 6

##利用訓練資料擬合模型
##BATCH_SIZE為梯度下降法每更新一次參數值所跑完的資料數目；NUM_EPOCHS為RNN模型迭代次數

history = model.fit(
    x=x_train, 
    y=y_train,
    batch_size=BATCH_SIZE,
    epochs=NUM_EPOCHS,
    
    validation_data=(
        x_val, 
        y_val
    ),
    
    shuffle=True
)

##利用同樣的方法輸入01_Hoteltest並轉換為模型內可用數值

corpos_test = pd.DataFrame({
    'review': data_test['review'], 
    'label': data_test['label'],
    'segment': 'null'
    })

for i in range(len(corpos_test['review'])):
    segments = []
    segment = []
    label = corpos_test['label'][i]
    review = corpos_test['review'][i]
    segs = jieba.cut(review)
    for seg in segs:
        segs = []
        if seg not in stopwords:
            segs = segments.append(seg)
    
    corpos_test['segment'][i] = segments
    
segment_test = pd.DataFrame({
    'segment': corpos_test['segment'], 
    'label': corpos_test['label']
    })


corpus_test = segment_test.segment

tokenizer.fit_on_texts(corpus_test)
x_test = tokenizer.texts_to_sequences(corpus_test)

x_test = keras.preprocessing.sequence.pad_sequences(
    x_test,
    maxlen=MAX_SEQUENCE_LENGTH)

##利用上面建立好的模型預測測試集
##輸出預測結果

predictions = model.predict(x_test)
print(predictions[:10])

data_test['label'] = np.argmax(predictions, axis=1)

submission = data_test.loc[:, ['label']].reset_index()

submission.columns = ['index','label']
print(submission[:10])

##將預測結果存成csv檔

submission.to_csv('C:/Users/carter840826/01_Hoteltest(hsu).csv')

data_test.head()
