{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  label                                             review\n",
      "0      1      0  來往虹橋機場,絕對方便,938公共汽車直接門對門,上車睡覺就可以了(一個半小時)賓館服務不專...\n",
      "1      2      1  來武漢出差很多次了，這次經朋友介紹，住在海怡錦江大酒店。外表看來的確一般，但住進去後，感覺還...\n",
      "2      3      0  來長春之前，查閱了攜程的很多酒店評分，感覺這個還算不錯的，但是入住之後感覺很失望：1、房間門...\n",
      "3      4      1  來蘇州好多次了，基本上都住在這家酒店。給我的感覺非常好，房間很乾淨，設施也很全，各種用品也很...\n",
      "4      5      1  依海而建，地理位置絕佳，旁邊就是星海廣場。床特別大，很舒適，房間裡有咖啡包，但有一天服務生忘...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 3 columns):\n",
      "index     5000 non-null int64\n",
      "label     5000 non-null int64\n",
      "review    5000 non-null object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 117.3+ KB\n",
      "None\n",
      "----------\n",
      "Train columns with null values:\n",
      " index     0\n",
      "label     0\n",
      "review    0\n",
      "dtype: int64\n",
      "----------\n",
      "Test columns with null values:\n",
      " index     0\n",
      "label     0\n",
      "review    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "##利用pandas讀取資料\n",
    "\n",
    "data_train = pd.read_csv('C:/Users/carter840826/01_Hoteltrain.csv')\n",
    "data_test = pd.read_csv('C:/Users/carter840826/01_Hoteltest.csv')\n",
    "\n",
    "##複製一個訓練資料\n",
    "\n",
    "data1 = data_train.copy(deep = True)\n",
    "\n",
    "\n",
    "##檢查訓練、驗證資料資訊及有無空白\n",
    "\n",
    "print(data1.head())\n",
    "print(data1.info())\n",
    "print(\"-\"*10)\n",
    "print('Train columns with null values:\\n', data_train.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print('Test columns with null values:\\n', data_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\CARTER~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.139 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "C:\\Users\\carter840826\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                segment  label\n",
      "0     [往虹橋, 機場, 方便, 938, 公共, 汽車, 直接, 門對門, 上車, 睡覺, 一個...      0\n",
      "1     [武漢, 出差, 這次經, 朋友, 介紹, 住, 海怡錦江, 大酒店, 外表, 住, 後, ...      1\n",
      "2     [來長, 春, 之前, 查閱, 攜程, 酒店, 評分, 感覺, 還算, 不錯, 入住, 之後...      0\n",
      "3     [蘇州, 好, 住, 這家, 酒店, 給我, 感覺, 好, 房間, 乾淨, 設施, 全, 用...      1\n",
      "4     [依海而建, 地理位置, 絕佳, 旁邊, 星海, 廣場, 床特別, 很舒適, 房間裡, 咖啡...      1\n",
      "...                                                 ...    ...\n",
      "4995  [攜程, 推薦, 總台, 登記, 後, 房間, 掃, 大堂, 一下, 一小, 時, 沒人, ...      0\n",
      "4996  [攜程, 推薦, 星級, 酒店, 實際, 感覺, 房間裡, 招待所, 小不說, 髒, 亂, ...      0\n",
      "4997  [攜程, 這次, 不錯, 訂, 行政, 房給, 免費, 升級, 號樓, 山景, 房, 酒店,...      1\n",
      "4998  [攜程, 提供, 160, 元, 普通, 標間, 競爭力, 房間, 設施, 床單, 破損, ...      0\n",
      "4999  [攜程, 當天, 預定, 房間, 個小時, 不到, 時間, 達, 酒店, 卻, 告知, 已經...      0\n",
      "\n",
      "[5000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "##載入中文斷詞\n",
    "\n",
    "fp = open(r'C:/Users/carter840826/Chinese stopwords.txt',encoding='utf-8')\n",
    "stopwords = fp.readlines()\n",
    "stopwords = [w.replace('\\n', '') for w in stopwords]\n",
    "\n",
    "##將訓練資料集合成新的DataFarme，多添加segment之後放入處理過的句子\n",
    "\n",
    "corpos = pd.DataFrame({\n",
    "    'review': data1['review'], \n",
    "    'label': data1['label'],\n",
    "    'segment': 'null'\n",
    "    })\n",
    "\n",
    "##載入jieba，利用for迴圈處理每個句子、分詞以及移除斷詞\n",
    "##由於電腦速度問題，這邊無法使用posseg更精確地以詞性分詞，可能會造成之後正確率的問題\n",
    "\n",
    "for i in range(len(corpos['review'])):\n",
    "    segments = []\n",
    "    segment = []\n",
    "    label = corpos['label'][i]\n",
    "    review = corpos['review'][i]\n",
    "    segs = jieba.cut(review)\n",
    "    for seg in segs:\n",
    "        segs = []\n",
    "        if seg not in stopwords:\n",
    "            segs = segments.append(seg)\n",
    "    \n",
    "    corpos['segment'][i] = segments\n",
    "    \n",
    "##將處理好的分詞與label組成新的DataFrame，print出來看看成果\n",
    "    \n",
    "segmentDataFrame = pd.DataFrame({\n",
    "    'segment': corpos['segment'], \n",
    "    'label': corpos['label']\n",
    "    })\n",
    "print(segmentDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             segment\n",
      "0  [往虹橋, 機場, 方便, 938, 公共, 汽車, 直接, 門對門, 上車, 睡覺, 一個...\n",
      "1  [武漢, 出差, 這次經, 朋友, 介紹, 住, 海怡錦江, 大酒店, 外表, 住, 後, ...\n",
      "2  [來長, 春, 之前, 查閱, 攜程, 酒店, 評分, 感覺, 還算, 不錯, 入住, 之後...\n",
      "3  [蘇州, 好, 住, 這家, 酒店, 給我, 感覺, 好, 房間, 乾淨, 設施, 全, 用...\n",
      "4  [依海而建, 地理位置, 絕佳, 旁邊, 星海, 廣場, 床特別, 很舒適, 房間裡, 咖啡...\n",
      "5000 5000\n",
      "21 [10593, 156, 14, 10594, 2078] ...\n",
      "124 [5263, 294, 10597, 80, 426] ...\n",
      "26 [5266, 2420, 347, 10610, 20] ...\n",
      "95 [1156, 3, 7, 56, 1] ...\n",
      "39 [10614, 110, 2422, 195, 3720] ...\n",
      "447\n",
      "36.3958\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0 10593   156    14 10594  2078   727   251 10595  2419   405\n",
      "     19   832    23     6   578    54  6881   185   176   113 10596]\n",
      " [  137     7    65     1   156   949 10608    53   268     5   156   949\n",
      "   1365   918   156   169  1366   407     1 10609   580     1   580  1814\n",
      "     14     2   174   119    37    24     5     1   126   655   678]\n",
      " [    0     0     0     0     0     0     0     0     0  5266  2420   347\n",
      "  10610    20     1  1548    11   117     4     8  6886   253     2  6887\n",
      "     93    39     2   119    37     6  6888   728   327    10   117]\n",
      " [ 1636  2421  1417   400  1727  1080  1549   265   227  3719 10612    17\n",
      "     23    84    36    26    22    25    89     6   252  1484  5267  2946\n",
      "     89   183   221     2   836   314   837 10613   355   120   269]\n",
      " [ 3720   548 10615  1637   147   448  1036   234   293   559   449    10\n",
      "    103    46 10616  6889 10617 10618  5268   196  3721   859 10619  3722\n",
      "  10620  3723  3722   103 10621  6890 10622   950  1316  1939   448]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "##利用keras將文字轉換為相對應的數值，MAX_NUM_WORDS代表轉換字典內最多能包含的數目\n",
    "\n",
    "MAX_NUM_WORDS = 12500\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "corpus_x = segmentDataFrame.segment\n",
    "print(pd.DataFrame(corpus_x.iloc[:5],columns=['segment']))\n",
    "\n",
    "##將處理好的分詞套入上面設定的tokenizer建立字典並轉換為相對應的數值\n",
    "##print x_train與y_train的長度以確定資料量\n",
    "\n",
    "tokenizer.fit_on_texts(corpus_x)\n",
    "x_train = tokenizer.texts_to_sequences(corpus_x)\n",
    "y_train = segmentDataFrame.label\n",
    "print(len(x_train),len(y_train))\n",
    "\n",
    "##列出每句話的轉換後字串長度，並列出前5個數值\n",
    "##計算字串長度的max值以及mean值\n",
    "\n",
    "for seq in x_train[:5]:\n",
    "    print(len(seq), seq[:5], '...')\n",
    "max_seq_len = max([len(seq) for seq in x_train])\n",
    "mean_seq_len = sum([len(seq) for seq in x_train])/len([len(seq) for seq in x_train])\n",
    "print(max_seq_len)\n",
    "print(mean_seq_len)\n",
    "\n",
    "##將字串用Zero Padding設定在同樣的字串長度=35\n",
    "##不足的在前方以0補齊\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train,\n",
    "    maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "##將y_train轉換為One-hot Encoding，以便之後在決定答案時有機率值作依據\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "\n",
    "##再確定一次轉換後的句子數值以及答案數值\n",
    "\n",
    "print(x_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "x_train : (4500, 35)\n",
      "y_train : (4500, 2)\n",
      "----------\n",
      "Validation Set\n",
      "----------\n",
      "x_val :   (500, 35)\n",
      "y_val :   (500, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##利用sklearn裡面的train_test_split切割訓練資料\n",
    "##VALIDATION_RATIO=0.1代表切割資料比率為9:1\n",
    "\n",
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 10\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    test_size=VALIDATION_RATIO, \n",
    "    random_state=RANDOM_STATE)\n",
    "\n",
    "##確認訓練集合(Trainning Set)以及驗證集合(Validation Set)大小\n",
    "\n",
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x_train : {x_train.shape}\")\n",
    "print(f\"y_train : {y_train.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Validation Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x_val :   {x_val.shape}\")\n",
    "print(f\"y_val :   {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 35)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 35, 256)           3200000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 3,397,378\n",
      "Trainable params: 3,397,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "##利用keras內的模組建立RNN模型，RNN為有記憶力的循環網路，會依序讀取list裡面的值以後，計算並做出回饋\n",
    "##利用LSTM作為RNN模型內的細胞，LSTM的優點為可以將記憶狀態儲存\n",
    "##其中NUM_CLASSES為答案種類(0,1)；NUM_EMBEDDING_DIM為一個詞向量的維度；NUM_LSTM_UNITS為LSTM輸出的向量維度\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "MAX_NUM_WORDS = 12500\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "NUM_LSTM_UNITS = 128\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "\n",
    "##設定模型內的Input data以及Output data\n",
    "##經過詞嵌入層的轉換，每則評論都變成一個詞向量的序列\n",
    "##而每個詞向量的維度為256\n",
    "\n",
    "data_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "embedding_layer = Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "data_embedded = embedding_layer(data_input)\n",
    "\n",
    "##評論經過LSTM層後轉換為一個128維度的向量\n",
    "\n",
    "shared_lstm = LSTM(NUM_LSTM_UNITS)\n",
    "data_output = shared_lstm(data_embedded)\n",
    "\n",
    "##利用Softmax Activation傳回兩個值，分別代表可能的機率\n",
    "\n",
    "dense =  Dense(units=NUM_CLASSES, activation='softmax')\n",
    "predictions = dense(data_output)\n",
    "\n",
    "#建立模型\n",
    "\n",
    "model = Model(inputs=data_input, outputs=predictions)\n",
    "print(model.summary())\n",
    "\n",
    "##定義損失函數的模型\n",
    "##binary_crossentropy為損失函數內的交叉熵；accuracy可以用來了解模型正確度\n",
    "##優化器選擇rmsprop\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "4500/4500 [==============================] - 9s 2ms/sample - loss: 0.4951 - accuracy: 0.7573 - val_loss: 0.3677 - val_accuracy: 0.8140\n",
      "Epoch 2/6\n",
      "4500/4500 [==============================] - 4s 991us/sample - loss: 0.2508 - accuracy: 0.9060 - val_loss: 0.3665 - val_accuracy: 0.8380\n",
      "Epoch 3/6\n",
      "4500/4500 [==============================] - 4s 980us/sample - loss: 0.1549 - accuracy: 0.9460 - val_loss: 0.4498 - val_accuracy: 0.8740\n",
      "Epoch 4/6\n",
      "4500/4500 [==============================] - 4s 981us/sample - loss: 0.0913 - accuracy: 0.9691 - val_loss: 0.4986 - val_accuracy: 0.8440\n",
      "Epoch 5/6\n",
      "4500/4500 [==============================] - 5s 1ms/sample - loss: 0.0587 - accuracy: 0.9818 - val_loss: 0.6316 - val_accuracy: 0.8480\n",
      "Epoch 6/6\n",
      "4500/4500 [==============================] - 5s 1ms/sample - loss: 0.0369 - accuracy: 0.9873 - val_loss: 0.7152 - val_accuracy: 0.8480\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 6\n",
    "\n",
    "##利用訓練資料擬合模型\n",
    "##BATCH_SIZE為梯度下降法每更新一次參數值所跑完的資料數目；NUM_EPOCHS為RNN模型迭代次數\n",
    "\n",
    "history = model.fit(\n",
    "    x=x_train, \n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    \n",
    "    validation_data=(\n",
    "        x_val, \n",
    "        y_val\n",
    "    ),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carter840826\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "##利用同樣的方法輸入01_Hoteltest並轉換為模型內可用數值\n",
    "\n",
    "corpos_test = pd.DataFrame({\n",
    "    'review': data_test['review'], \n",
    "    'label': data_test['label'],\n",
    "    'segment': 'null'\n",
    "    })\n",
    "\n",
    "for i in range(len(corpos_test['review'])):\n",
    "    segments = []\n",
    "    segment = []\n",
    "    label = corpos_test['label'][i]\n",
    "    review = corpos_test['review'][i]\n",
    "    segs = jieba.cut(review)\n",
    "    for seg in segs:\n",
    "        segs = []\n",
    "        if seg not in stopwords:\n",
    "            segs = segments.append(seg)\n",
    "    \n",
    "    corpos_test['segment'][i] = segments\n",
    "    \n",
    "segment_test = pd.DataFrame({\n",
    "    'segment': corpos_test['segment'], \n",
    "    'label': corpos_test['label']\n",
    "    })\n",
    "\n",
    "\n",
    "corpus_test = segment_test.segment\n",
    "\n",
    "tokenizer.fit_on_texts(corpus_test)\n",
    "x_test = tokenizer.texts_to_sequences(corpus_test)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_test,\n",
    "    maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.6565495e-01 6.3434500e-01]\n",
      " [2.3280169e-01 7.6719832e-01]\n",
      " [8.2911383e-03 9.9170882e-01]\n",
      " [2.3722029e-03 9.9762779e-01]\n",
      " [4.5220327e-02 9.5477962e-01]\n",
      " [9.7592825e-01 2.4071775e-02]\n",
      " [1.3780651e-05 9.9998617e-01]\n",
      " [5.2148625e-05 9.9994791e-01]\n",
      " [9.4583929e-01 5.4160729e-02]\n",
      " [3.0432528e-01 6.9567472e-01]]\n",
      "   index  label\n",
      "0      0      1\n",
      "1      1      1\n",
      "2      2      1\n",
      "3      3      1\n",
      "4      4      1\n",
      "5      5      0\n",
      "6      6      1\n",
      "7      7      1\n",
      "8      8      0\n",
      "9      9      1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\"此期間預訂，入住首日酒店贈送每間房10元洗衣券一張，通過攜程預訂，入住首日每房還可獲贈歡迎...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>&amp;#35828;&amp;#23454;&amp;#35805;，&amp;#23545;景&amp;#21306;酒店的硬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>(1)房間衛生乾淨空間大!(2)早餐美味風富菜色多!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>（1）酒店冊子介紹說房間內提供飲用水，水壺內沒有水，給前臺提意見。前臺說飲用水就是衛生間的自...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>*房間很不錯，服務很好，就是位置偏點，在機場到市區的路邊，打車到江北商業圈起步價。*早餐不錯。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  label                                             review\n",
       "0      1      1  \"此期間預訂，入住首日酒店贈送每間房10元洗衣券一張，通過攜程預訂，入住首日每房還可獲贈歡迎...\n",
       "1      2      1  &#35828;&#23454;&#35805;，&#23545;景&#21306;酒店的硬...\n",
       "2      3      1                         (1)房間衛生乾淨空間大!(2)早餐美味風富菜色多!\n",
       "3      4      1  （1）酒店冊子介紹說房間內提供飲用水，水壺內沒有水，給前臺提意見。前臺說飲用水就是衛生間的自...\n",
       "4      5      1    *房間很不錯，服務很好，就是位置偏點，在機場到市區的路邊，打車到江北商業圈起步價。*早餐不錯。"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##利用上面建立好的模型預測測試集\n",
    "##輸出預測結果\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "print(predictions[:10])\n",
    "\n",
    "data_test['label'] = np.argmax(predictions, axis=1)\n",
    "\n",
    "submission = data_test.loc[:, ['label']].reset_index()\n",
    "\n",
    "submission.columns = ['index','label']\n",
    "print(submission[:10])\n",
    "\n",
    "##將預測結果存成csv檔\n",
    "\n",
    "submission.to_csv('C:/Users/carter840826/01_Hoteltest(hsu).csv')\n",
    "\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
